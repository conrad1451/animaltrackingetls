# etl_script.py

# CHQ: generated by Gemini AI

import os
import time
import logging
import requests
import json
import pandas as pd
from datetime import datetime
from dateutil.parser import parse as parse_date # For robust date parsing

# Import tenacity for retry logic
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type

# --- Configure Logging ---
# This will print informative messages to your console when the script runs
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration for ETL ---
GBIF_BASE_URL = 'https://api.gbif.org/v1'

# Default parameters for extraction if not specified by the caller
# You can change these defaults or pass them dynamically when calling run_monarch_etl
DEFAULT_TAXON_KEY = '5133088' # Monarch Butterfly
DEFAULT_COUNTRY_CODE = 'US'
DEFAULT_LIMIT_PER_PAGE = 300 # GBIF max limit for /occurrence/search is 300, more efficient for ETL

# --- Retry Decorator for API Calls ---
@retry(
    wait=wait_exponential(multiplier=1, min=2, max=10), # Wait 2s, then 4s, then 8s, up to 10s
    stop=stop_after_attempt(5),                          # Try fetching up to 5 times
    retry=retry_if_exception_type((
        requests.exceptions.ConnectionError, # For network connectivity issues
        requests.exceptions.Timeout,         # For request timeouts
        requests.exceptions.HTTPError        # For HTTP errors (e.g., 5xx server errors, rate limits)
    )),
    reraise=True # Re-raise the last exception if all retries fail
)
def fetch_gbif_page_etl(endpoint, params):
    """
    Fetches a single page of data from the GBIF API with retry logic.
    Raises HTTPError for bad responses (4xx or 5xx) that `tenacity` can catch.
    """
    logger.info(f"Attempting to fetch data from: {endpoint} with params: {params}")
    response = requests.get(endpoint, params=params)
    response.raise_for_status() # This will raise an HTTPError for 4xx/5xx responses
    return response.json()

# --- Extract Function ---
def extract_gbif_data(taxon_key=DEFAULT_TAXON_KEY, country_code=DEFAULT_COUNTRY_CODE, **kwargs):
    """
    Extracts all paginated occurrence data for a given taxon and country from GBIF API.
    
    Args:
        taxon_key (str): The GBIF taxon key for the species (e.g., '5133088' for Monarch).
        country_code (str): The 2-letter ISO country code (e.g., 'US').
        **kwargs: Additional GBIF API parameters like 'year', 'month', 'day'.
    
    Returns:
        list: A list of dictionaries, where each dictionary is a raw GBIF occurrence record.
    """
    all_gbif_records = []
    current_offset = 0
    end_of_records = False
    
    # Start with base parameters and add any from kwargs
    params = {
        'taxonKey': taxon_key,
        'country': country_code,
        'hasCoordinate': 'true',         # Only records with coordinates
        'hasGeospatialIssue': 'false',   # Exclude records with geospatial issues
        'limit': DEFAULT_LIMIT_PER_PAGE  # Number of records per page
    }
    params.update(kwargs) # Add year, month, day if present in kwargs

    endpoint = GBIF_BASE_URL + "/occurrence/search"
    
    logger.info(f"Starting extraction for: {params}")

    # Loop to handle pagination
    while not end_of_records:
        page_params = params.copy() # Create a copy to modify offset for each page
        page_params['offset'] = current_offset

        try:
            page_data = fetch_gbif_page_etl(endpoint, page_params)
            
            records_on_page = page_data.get('results', [])
            all_gbif_records.extend(records_on_page) # Add current page's records to the total list
            
            end_of_records = page_data.get('endOfRecords', True) # Check if this is the last page
            
            # Update offset for the next page, using actual 'limit' from response if available
            current_offset += page_data.get('limit', 0) 
            
            logger.info(f"Fetched {len(records_on_page)} records. Total: {len(all_gbif_records)}. Next offset: {current_offset}. End of records: {end_of_records}")
            
            # Add a small delay between requests to be polite to the API, but only if more pages are expected
            if not end_of_records and len(records_on_page) > 0:
                 time.sleep(0.5) # Wait 0.5 seconds between page requests

        except requests.exceptions.RequestException as e:
            logger.error(f"Critical extraction error for offset {current_offset}: {e}. Stopping extraction.")
            # Depending on your needs, you might want to save partially extracted data here
            break # Stop processing if extraction fails for a page after all retries
        except json.JSONDecodeError:
            logger.error(f"Invalid JSON response from GBIF for offset {current_offset}. Skipping this page.")
            # This might indicate a malformed response, consider if you want to break or continue
            current_offset += params['limit'] # Advance offset to try the next page
            continue # Continue to the next iteration

    logger.info(f"Finished extraction. Total raw records extracted: {len(all_gbif_records)}")
    return all_gbif_records

# --- Transform Function ---
def transform_gbif_data(raw_data):
    """
    Transforms raw GBIF occurrence data into a cleaned and enriched Pandas DataFrame.
    
    Args:
        raw_data (list): A list of dictionaries, where each is a raw GBIF occurrence record.
        
    Returns:
        pandas.DataFrame: A DataFrame with cleaned and selected occurrence data.
    """
    if not raw_data:
        logger.warning("No raw data to transform.")
        return pd.DataFrame()

    logger.info(f"Starting transformation of {len(raw_data)} records.")
    
    # Convert list of dictionaries to DataFrame
    df = pd.DataFrame(raw_data)

    # --- Data Cleaning & Standardization ---
    
    # 1. Robust Date Parsing for 'eventDate'
    df['eventDateParsed'] = pd.NaT # Initialize new column with "Not a Time"
    for index, row in df.iterrows():
        date_str = row.get('eventDate')
        if date_str:
            try:
                # dateutil.parser.parse is robust enough to handle "YYYY", "YYYY-MM", "YYYY-MM-DD", "YYYY-MM-DDTHH:MM:SS"
                df.at[index, 'eventDateParsed'] = parse_date(date_str)
            except (ValueError, TypeError) as e:
                logger.debug(f"Could not parse eventDate '{date_str}' at index {index}. Error: {e}")
                # Keep pd.NaT for unparseable dates

    # Remove rows where eventDate could not be parsed, or handle them specifically
    df.dropna(subset=['eventDateParsed'], inplace=True)
    logger.info(f"After date parsing: {len(df)} records.")

    # 2. Convert coordinates to numeric, coercing errors to NaN
    df['decimalLatitude'] = pd.to_numeric(df['decimalLatitude'], errors='coerce')
    df['decimalLongitude'] = pd.to_numeric(df['decimalLongitude'], errors='coerce')

    # Drop rows with invalid (NaN) coordinates if they are essential for your analysis
    df.dropna(subset=['decimalLatitude', 'decimalLongitude'], inplace=True)
    logger.info(f"After dropping records without valid coordinates: {len(df)} records.")

    # 3. Handle 'individualCount': Coerce to numeric, fill NaN with 1 (common for occurrence records)
    df['individualCount'] = pd.to_numeric(df['individualCount'], errors='coerce').fillna(1).astype(int)

    # --- Enrichment / Feature Engineering ---
    # Derive date components if 'eventDateParsed' is available
    if not df['eventDateParsed'].empty: # Check if the column has valid dates
        df['year'] = df['eventDateParsed'].dt.year
        df['month'] = df['eventDateParsed'].dt.month
        df['day'] = df['eventDateParsed'].dt.day
        df['day_of_week'] = df['eventDateParsed'].dt.dayofweek # Monday=0, Sunday=6
        df['week_of_year'] = df['eventDateParsed'].dt.isocalendar().week.astype(int) # ISO week number
        df['date_only'] = df['eventDateParsed'].dt.date # For simple date comparisons (Python date object)
    else:
        # If no dates could be parsed, ensure these columns still exist (e.g., with NaNs)
        df['year'] = pd.NA
        df['month'] = pd.NA
        df['day'] = pd.NA
        df['day_of_week'] = pd.NA
        df['week_of_year'] = pd.NA
        df['date_only'] = pd.NaT

    # --- Select and Reorder Relevant Columns ---
    # Define the columns you want in your final dataset.
    # This list should reflect the fields you find most useful from GBIF and your derived fields.
    final_columns = [
        'gbifID', 'datasetKey', 'datasetName', 'publishingOrgKey', 'publishingOrganizationTitle',
        'eventDate', 'eventDateParsed', 'year', 'month', 'day', 'day_of_week', 'week_of_year', 'date_only',
        'scientificName', 'vernacularName', 'taxonKey', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species',
        'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters',
        'countryCode', 'stateProvince', 'county', 'locality',
        'individualCount', 'basisOfRecord', 'recordedBy', 'occurrenceID', 'collectionCode', 'catalogNumber'
        # Add other fields from GBIF you might need, e.g., 'recordedBy', 'identificationID', etc.
    ]
    
    # Filter columns to only include those that actually exist in the DataFrame
    # This prevents errors if a field isn't present in all GBIF records
    df_transformed = df[[col for col in final_columns if col in df.columns]].copy()
    
    logger.info(f"Finished transformation. Transformed records: {len(df_transformed)}.")
    return df_transformed

# --- Load Function (Example: to CSV or SQLite) ---
def load_data(transformed_data_df, destination_type='csv', file_path='monarch_sightings.csv'):
    """
    Loads transformed data to a specified destination.
    
    Args:
        transformed_data_df (pandas.DataFrame): The DataFrame containing the cleaned data.
        destination_type (str): 'csv' or 'sqlite'.
        file_path (str): Path to the CSV file or SQLite database.
    """
    if transformed_data_df.empty:
        logger.warning("No data to load as DataFrame is empty.")
        return

    logger.info(f"Starting loading of {len(transformed_data_df)} records to {destination_type} at {file_path}.")

    try:
        if destination_type == 'csv':
            # Append mode ('a') will add to the file. 'header=not os.path.exists(file_path)' writes header only once.
            # This is simple for demonstration but can lead to duplicate headers if file is empty then appended.
            # For robust appending, you'd load existing CSV, concat, then overwrite.
            transformed_data_df.to_csv(file_path, index=False, mode='a', header=not os.path.exists(file_path))
            logger.info(f"Data appended to CSV file: {file_path}")
        
        elif destination_type == 'sqlite':
            import sqlite3 # Import here to avoid requiring sqlite3 if not used
            conn = sqlite3.connect(file_path)
            # if_exists='append' adds new records. Use 'replace' for a full refresh of the table.
            # Primary key (gbifID) should prevent duplicates if set in DB schema.
            transformed_data_df.to_sql('monarch_sightings', conn, if_exists='append', index=False)
            conn.close()
            logger.info(f"Data appended to SQLite database: {file_path}, table: monarch_sightings")
        
        else:
            logger.error(f"Unsupported destination type: {destination_type}")

    except Exception as e:
        logger.error(f"Error during data loading to {destination_type}: {e}")

# --- Main ETL Execution Function ---
def run_monarch_etl(target_year=None, target_month=None, target_day=None,
                    output_file='monarch_sightings.csv', output_type='csv'):
    """
    Runs the full ETL process for monarch butterfly sightings.
    
    Args:
        target_year (int, optional): Year to filter data for.
        target_month (int, optional): Month to filter data for.
        target_day (int, optional): Day to filter data for.
        output_file (str): The name/path of the output file/database.
        output_type (str): 'csv' or 'sqlite'.
    """
    logger.info("--- ETL process started ---")
    
    # Build query parameters for extraction
    extract_params = {}
    if target_year:
        extract_params['year'] = str(target_year)
    if target_month:
        extract_params['month'] = str(target_month)
    if target_day:
        extract_params['day'] = str(target_day)

    # 1. Extract
    raw_data = extract_gbif_data(
        taxon_key=DEFAULT_TAXON_KEY,
        country_code=DEFAULT_COUNTRY_CODE,
        **extract_params # Pass year, month, day to the GBIF API call
    )

    # 2. Transform
    transformed_df = transform_gbif_data(raw_data)

    # 3. Load
    load_data(transformed_df, destination_type=output_type, file_path=output_file)

    logger.info("--- ETL process finished ---")


if __name__ == '__main__':
    # --- ETL Execution Examples ---
    # You would typically run this script using a scheduler (e.g., cron job)
    # and pass parameters based on your scheduling needs (e.g., fetch data for yesterday).

    # Example 1: Fetch data for a specific day
    logger.info("\nRunning ETL for 2024-07-14")
    run_monarch_etl(target_year=2024, target_month=7, target_day=14,
                    output_file='monarch_sightings_20240714.csv', output_type='csv')

    # Example 2: Fetch data for an entire month (be aware of data volume!)
    # GBIF API allows 'year' and 'month' parameters directly.
    logger.info("\nRunning ETL for 2024-06 (June 2024)")
    run_monarch_etl(target_year=2024, target_month=6,
                    output_file='monarch_sightings_202406.csv', output_type='csv')
    
    # Example 3: Fetch all data for a specific year (potentially very large)
    # logger.info("\nRunning ETL for all of 2023 (WARNING: This can be a very large dataset!)")
    # run_monarch_etl(target_year=2023,
    #                 output_file='monarch_sightings_2023_full.csv', output_type='csv')

    # Example 4: Load to SQLite database instead of CSV
    # logger.info("\nRunning ETL for 2024-01-01 to SQLite")
    # run_monarch_etl(target_year=2024, target_month=1, target_day=1,
    #                 output_file='monarch_sightings.db', output_type='sqlite')