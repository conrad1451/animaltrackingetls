# etl_script.py

# CHQ: generated by Gemini AI

import os
import time
import logging
import requests
import json
import pandas as pd
from datetime import datetime, timedelta # Add timedelta here
from dateutil.parser import parse as parse_date # For robust date parsing
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type

# For PostgreSQL connection
from sqlalchemy import create_engine
import psycopg2 # Imported by create_engine, but good to have for type hinting/error messages

# --- Configure Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration for ETL ---
GBIF_BASE_URL = 'https://api.gbif.org/v1'
DEFAULT_TAXON_KEY = '5133088' # Monarch Butterfly
DEFAULT_COUNTRY_CODE = 'US'
DEFAULT_LIMIT_PER_PAGE = 300 # GBIF max limit for /occurrence/search is 300, more efficient for ETL

# --- Neon Database Configuration (READ FROM ENVIRONMENT VARIABLES) ---
NEON_DB_HOST = os.getenv('NEON_DB_HOST')
NEON_DB_NAME = os.getenv('NEON_DB_NAME')
NEON_DB_USER = os.getenv('NEON_DB_USER')
NEON_DB_PASSWORD = os.getenv('NEON_DB_PASSWORD')
NEON_DB_PORT = os.getenv('NEON_DB_PORT', '5432') # Default to 5432 if not set

AI_ENDPOINT_BASE_URL = os.getenv('AI_ENDPOINT_BASE_URL')


# --- Retry Decorator for API Calls ---
@retry(
    wait=wait_exponential(multiplier=1, min=2, max=10),
    stop=stop_after_attempt(5),
    retry=retry_if_exception_type((
        requests.exceptions.ConnectionError,
        requests.exceptions.Timeout,
        requests.exceptions.HTTPError
    )),
    reraise=True
)
def fetch_gbif_page_etl(endpoint, params):
    """
    Fetches a single page of data from the GBIF API with retry logic.
    """
    logger.info(f"Attempting to fetch data from: {endpoint} with params: {params}")
    response = requests.get(endpoint, params=params)
    response.raise_for_status()
    return response.json()

# takes the input latitude, longitude, and location uncertainty for a single sighting
# and returns the county and city/town that the sighting occured in
def fetch_ai_county_city_town_analysis(latitude, longitude, uncertainty):
    endpoint = AI_ENDPOINT_BASE_URL

    the_endpoint = endpoint + "/countycityfromcoordinates?latitude="
    the_endpoint = the_endpoint + str(latitude) + "&longitude=" + str(longitude) + "&coordinate_uncertainty=" + str(uncertainty)
    """
    Fetches a single page of data from the GBIF API with retry logic.
    """
    logger.info(f"Attempting to fetch data from: {endpoint}")
    # logger.info(f"Attempting to fetch data from: {endpoint} with params: {params}")
    response = requests.get(the_endpoint)
    # response = requests.get(endpoint, params=params)

    response.raise_for_status()
    return response.json()

 
# --- Extract Function ---
def extract_gbif_data(taxon_key=DEFAULT_TAXON_KEY, country_code=DEFAULT_COUNTRY_CODE, **kwargs):
    """
    Extracts all paginated occurrence data for a given taxon and country from GBIF API.
    """
    all_gbif_records = []
    current_offset = 0
    end_of_records = False
    
    params = {
        'taxonKey': taxon_key,
        'country': country_code,
        'hasCoordinate': 'true',
        'hasGeospatialIssue': 'false',
        'limit': DEFAULT_LIMIT_PER_PAGE
    }
    params.update(kwargs)

    endpoint = GBIF_BASE_URL + "/occurrence/search"
    
    logger.info(f"Starting extraction for: {params}")

    while not end_of_records:
        page_params = params.copy()
        page_params['offset'] = current_offset

        try:
            page_data = fetch_gbif_page_etl(endpoint, page_params)
            
            records_on_page = page_data.get('results', [])
            all_gbif_records.extend(records_on_page)
            
            end_of_records = page_data.get('endOfRecords', True)
            current_offset += page_data.get('limit', 0)
            
            logger.info(f"Fetched {len(records_on_page)} records. Total: {len(all_gbif_records)}. Next offset: {current_offset}. End of records: {end_of_records}")
            
            if not end_of_records and len(records_on_page) > 0:
                    time.sleep(0.5)

        except requests.exceptions.RequestException as e:
            logger.error(f"Critical extraction error for offset {current_offset}: {e}. Stopping extraction.")
            break
        except json.JSONDecodeError:
            logger.error(f"Invalid JSON response from GBIF for offset {current_offset}. Skipping this page.")
            current_offset += params['limit']
            continue

    logger.info(f"Finished extraction. Total raw records extracted: {len(all_gbif_records)}")
    return all_gbif_records


# adds the county and city/town that a sighting occured in to the record of the sighting
# def transform_add_county_city_town_single_sighting(the_transformed_df, the_location_params):
def transform_add_county_city_town_single_sighting(the_transformed_df, the_index):

    # the_params['decimalLatitude'] = 0
    # the_params['decimalLongitude'] = 0
    # the_params['coordinateUncertaintyInMeters'] = 0

    # the_transformed_df[]

    x=the_index

    df = the_transformed_df
 
    county_and_city_for_single_sighting = fetch_ai_county_city_town_analysis(df.at[x, 'decimalLatitude'], df.at[x, 'decimalLongitude'], df.at[x, 'coordinateUncertaintyInMeters'])
    
    the_transformed_df['county'][x] = county_and_city_for_single_sighting['county']
    the_transformed_df['cityOrTown'][x] = county_and_city_for_single_sighting['city/town']

    return the_transformed_df

# CHQ: Gemini AI debugged this function and added helper function
# --- Transform Function ---
def transform_gbif_data(raw_data):
    """
    Transforms raw GBIF occurrence data into a cleaned and enriched Pandas DataFrame.
    """
    if not raw_data:
        logger.warning("No raw data to transform.")
        return pd.DataFrame()

    logger.info(f"Starting transformation of {len(raw_data)} records.")

    df = pd.DataFrame(raw_data)

    # 1. Robust Date Parsing for 'eventDate'
    df['eventDateParsed'] = pd.NaT
    for index, row in df.iterrows():
        date_str = row.get('eventDate')
        if date_str:
            try:
                df.at[index, 'eventDateParsed'] = parse_date(date_str)
            except (ValueError, TypeError) as e:
                logger.debug(f"Could not parse eventDate '{date_str}' at index {index}. Error: {e}")

    df.dropna(subset=['eventDateParsed'], inplace=True)
    logger.info(f"After date parsing: {len(df)} records.")

    # 2. Convert coordinates to numeric, coercing errors to NaN
    df['decimalLatitude'] = pd.to_numeric(df['decimalLatitude'], errors='coerce')
    df['decimalLongitude'] = pd.to_numeric(df['decimalLongitude'], errors='coerce')

    df.dropna(subset=['decimalLatitude', 'decimalLongitude'], inplace=True)
    logger.info(f"After dropping records without valid coordinates: {len(df)} records.")

    # 3. Handle 'individualCount': Coerce to numeric, fill NaN with 1
    df['individualCount'] = pd.to_numeric(df['individualCount'], errors='coerce').fillna(1).astype(int)

    # --- Enrichment / Feature Engineering ---
    if not df['eventDateParsed'].empty:
        df['year'] = df['eventDateParsed'].dt.year
        df['month'] = df['eventDateParsed'].dt.month
        df['day'] = df['eventDateParsed'].dt.day
        df['day_of_week'] = df['eventDateParsed'].dt.dayofweek
        df['week_of_year'] = df['eventDateParsed'].dt.isocalendar().week.astype(int)
        df['date_only'] = df['eventDateParsed'].dt.date
    else:
        df['year'] = pd.NA
        df['month'] = pd.NA
        df['day'] = pd.NA
        df['day_of_week'] = pd.NA
        df['week_of_year'] = pd.NA
        df['date_only'] = pd.NaT

    # Define the columns you want in your final dataset.
    # Ensure 'gbifID' is included as it's a good candidate for a primary key.
    # GBIF IDs can be very large, so storing as TEXT in PostgreSQL is safest to avoid precision issues.
    final_columns = [
        'gbifID', 'datasetKey', 'datasetName', 'publishingOrgKey', 'publishingOrganizationTitle',
        'eventDate', 'eventDateParsed', 'year', 'month', 'day', 'day_of_week', 'week_of_year', 'date_only',
        'scientificName', 'vernacularName', 'taxonKey', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species',
        'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters',
        'countryCode', 'stateProvince', 'locality', # 'county' and 'cityOrTown' will be added later
        'individualCount', 'basisOfRecord', 'recordedBy', 'occurrenceID', 'collectionCode', 'catalogNumber',
    ]

    # Select only the columns that exist in the DataFrame
    df_transformed = df[[col for col in final_columns if col in df.columns]].copy()

    # Convert gbifID to string to avoid potential precision loss in large integers when loading to DB
    if 'gbifID' in df_transformed.columns:
        df_transformed['gbifID'] = df_transformed['gbifID'].astype(str)

    # --- Add 'county' and 'cityOrTown' columns using the AI endpoint ---
    # Initialize 'county' and 'cityOrTown' columns with NaN or None before applying
    # This ensures the columns exist if the apply function doesn't return them for some rows.
    df_transformed['county'] = None
    df_transformed['cityOrTown'] = None

    # Define a helper function to call the AI endpoint for a single row
    # This function will be applied to each row using df.apply(axis=1)
    def get_county_city_for_row(row):
        try:
            latitude = row['decimalLatitude']
            longitude = row['decimalLongitude']
            uncertainty = row['coordinateUncertaintyInMeters'] if pd.notna(row['coordinateUncertaintyInMeters']) else 0 # Handle potential NaN uncertainty

            # Only call the AI endpoint if coordinates are valid
            if pd.notna(latitude) and pd.notna(longitude):
                result = fetch_ai_county_city_town_analysis(latitude, longitude, uncertainty)
                row['county'] = result.get('county')
                row['cityOrTown'] = result.get('city/town')
            else:
                row['county'] = None
                row['cityOrTown'] = None
        except Exception as e:
            logger.warning(f"Failed to get county/city for row (ID: {row.get('gbifID')}, Lat: {latitude}, Lon: {longitude}): {e}")
            row['county'] = None # Assign None on error
            row['cityOrTown'] = None # Assign None on error
        return row # Must return the modified row (as a Series)

    # Apply the function to each row of the DataFrame
    # This will iterate through each row, call your AI endpoint, and update the 'county' and 'cityOrTown' values
    logger.info(f"Calling AI endpoint for {len(df_transformed)} records to enrich location data...")
    df_transformed = df_transformed.apply(get_county_city_for_row, axis=1)
    logger.info("Finished enriching location data with AI endpoint.")

    logger.info(f"Finished transformation. Transformed records: {len(df_transformed)}.")
    return df_transformed

# The transform_add_county_city_town_single_sighting function is no longer needed
# as its logic is now embedded directly in the .apply() within transform_gbif_data.
# You can remove it, or keep it if you have other uses for it.
# If keeping it, note that it expects 'the_index', but the apply function works on rows.
# So, it's better to just embed the logic directly as shown above.
# --- Load Function (to Neon PostgreSQL) ---
def load_data(transformed_data_df, table_name='monarch_sightings'):
    """
    Loads transformed data to a PostgreSQL database (e.g., Neon).
    """
    if transformed_data_df.empty:
        logger.warning("No data to load as DataFrame is empty.")
        return

    # Check if all necessary environment variables are set
    if not all([NEON_DB_HOST, NEON_DB_NAME, NEON_DB_USER, NEON_DB_PASSWORD, NEON_DB_PORT]):
        logger.error("Database connection environment variables are not fully set. Cannot load data.")
        logger.error("Please set NEON_DB_HOST, NEON_DB_NAME, NEON_DB_USER, NEON_DB_PASSWORD, NEON_DB_PORT.")
        return

    # Construct the database connection string for SQLAlchemy
    # sslmode=require is crucial for Neon
    db_connection_str = (
        f"postgresql+psycopg2://{NEON_DB_USER}:{NEON_DB_PASSWORD}@"
        f"{NEON_DB_HOST}:{NEON_DB_PORT}/{NEON_DB_NAME}?sslmode=require"
    )

    logger.info(f"Starting loading of {len(transformed_data_df)} records to PostgreSQL table: {table_name}.")

    try:
        engine = create_engine(db_connection_str)
        
        # Use 'append' to add new records.
        # If you want to completely refresh the table each time, use 'replace'.
        # 'replace' will drop the table and recreate it.
        # 'if_exists' options: 'fail', 'replace', 'append'
        transformed_data_df.to_sql(table_name, engine, if_exists='append', index=False)
        
        logger.info(f"Data loaded successfully to PostgreSQL table: {table_name}")

    except Exception as e:
        logger.error(f"Error during data loading to PostgreSQL: {e}", exc_info=True)

# --- Main ETL Execution Function ---
def run_monarch_etl(target_year=None, target_month=None, target_day=None,
                    db_table_name='monarch_sightings'):
    """
    Runs the full ETL process for monarch butterfly sightings and loads to Neon PostgreSQL.
    """
    logger.info("--- ETL process started ---")
    
    # Build query parameters for extraction
    extract_params = {}
    if target_year:
        extract_params['year'] = str(target_year)
    if target_month:
        extract_params['month'] = str(target_month)
    if target_day:
        extract_params['day'] = str(target_day)

    # 1. Extract
    raw_data = extract_gbif_data(
        taxon_key=DEFAULT_TAXON_KEY,
        country_code=DEFAULT_COUNTRY_CODE,
        **extract_params
    )

    # 2. Transform
    transformed_df = transform_gbif_data(raw_data)

    # 3. Load
    load_data(transformed_df, table_name=db_table_name)

    logger.info("--- ETL process finished ---")

# CHQ: Gemin AI modified the main for the github actions job
if __name__ == '__main__':
    # When run by a scheduler, we typically want to process data for the *previous* full month.
    # This is more likely to yield data than a single day, especially for migratory species.

    # Calculate the date for the previous month
    today = datetime.now()
    # Go back to the first day of the current month, then subtract one day to get to the last day of the previous month
    first_day_of_current_month = today.replace(day=1)
    last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)

    target_year = last_day_of_previous_month.year
    target_month = last_day_of_previous_month.month

    logger.info(f"\nRunning ETL for {target_year}-{target_month} (entire month)")
    run_monarch_etl(target_year=target_year, target_month=target_month)

    # --- ETL Execution Examples (for reference) ---
    # Before running, ensure your Neon environment variables are set!
    # e.g., export NEON_DB_HOST="..." etc.

    # Example 1: Fetch data for a specific day (e.g., today) - less likely to find data
    # current_date = datetime.now()
    # logger.info(f"\nRunning ETL for {current_date.year}-{current_date.month}-{current_date.day}")
    # run_monarch_etl(target_year=current_date.year, target_month=current_date.month, target_day=current_date.day)

    # Example 2: Fetch data for an entire year (POTENTIALLY VERY LARGE - USE WITH CAUTION)
    # logger.info("\nRunning ETL for all of 2023 (WARNING: This can be a very large dataset!)")
    # run_monarch_etl(target_year=2023)